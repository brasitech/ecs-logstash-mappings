# Corelight ECS Logstash Pipeline
# Git Repository: https://github.com/corelight/ecs-logstash-mappings
# Authors: Corelight Inc, Brasi Tech LLC
# License: BSD 3-Clause
# Support: https://github.com/corelight/ecs-logstash-mappings/issues/new
# Releases: https://github.com/corelight/ecs-logstash-mappings/releases

input {
  kafka {
    ################## Topics ##################
    # Topic(s) to consume from
    #topics => [ "corelight" ]
    #:Example:# to consume topics based on regex. This will consume from all topics starting with "corelight_" except if followed by conn,ssl,files,x509,http,dns
    #topics_pattern => "corelight_(?!(conn|ssl|files|x509|http|dns)$).*" 
    #:Example:# to consume topics based on regex. This will consume from all topics starting with "corelight_" and if only followed by conn,ssl,files,x509,http,dns
    #topics_pattern => "corelight_(conn|ssl|files|x509|http|dns).*" 

    ################## Consumer Threads ##################
    # This setting should match the number of Kafka partitions for the topic(s) you are consuming from.
    # Change this if you have customized the number of kafka partitions for the topic AND are needing to scale (up) consumption
    # Read documentation for more info: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-kafka.html#plugins-inputs-kafka-consumer_threads
    consumer_threads => 3

    ################## Kafka Servers ##################
    #bootstrap_servers => "localhost:9092"
    #bootstrap_servers => "localhost:9092,localhost:9093,localhost:9094" #:Example:# kafka (cluster) with multiple nodes


    # Set to the naming convention that best suites your environment
    group_id => "logstash_corelight_group"
    # Set unique per logstash instance
    client_id => "logstash_consumer"


    auto_offset_reset => "earliest"
    decorate_events => "extended" #default: none
    enable_auto_commit => true
    max_poll_records => 500 #default: 500

    #fetch_min_bytes => "" #default: none
    #group_instance_id => "" #default: none
    auto_commit_interval_ms => 5000 #default: 5000 #5 seconds
    check_crcs => true #default: true
    client_dns_lookup => "default" #default: default
    connections_max_idle_ms => 540000 #default: 540000 #9 minutes
    fetch_max_bytes => 52428800 #default: 52428800 #50MB
    fetch_max_wait_ms => 500 #default: 500 #.5 seconds
    heartbeat_interval_ms => 3000 #default: 3000 #3 seconds
    max_partition_fetch_bytes => 1048576 #default: 1048576 #1MB
    max_poll_interval_ms => 300000 #default: 300000 #5 minutes
    metadata_max_age_ms => 300000 #default: 300000 #5 minutes
    partition_assignment_strategy => "" #default: "" #none
    poll_timeout_ms => 100 #default: 100 #.1 seconds
    receive_buffer_bytes => 32768 #default: 32768 #32KB
    reconnect_backoff_ms => 50 #default: 50 #.005 seconds
    request_timeout_ms => 40000 #default: 40000 #4 seconds
    retry_backoff_ms => 100 #default: 100 #.1 seconds
    security_protocol => "PLAINTEXT" #default: PLAINTEXT
    send_buffer_bytes => 131072 #default 131072 #128KB
    session_timeout_ms => 10000 #default: 10000 #10 seconds

    ################ Required fields, do not modify these ################
    codec => "json"
    add_field => {
      "[@metadata][etl][input_application_protocol]" => "kafka"
      "[@metadata][etl][format_is_json]" => "true"
      "[@metadata][etl][format_applied]" => "json"
      "[@metadata][etl][format_final_codec]" => "json"
      "[@metadata][z_no_reuse][event_type]" => "corelight"
      "[@metadata][etl][pipeline]" => "input-kafka-2dbbda4be01c-20220310.01"
    }
    ################ END Required fields, do not modify these ################
    id => "input-kafka-2dbbda4be01c"
  }
}
